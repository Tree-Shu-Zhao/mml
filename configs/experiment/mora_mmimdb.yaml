# @package _global_
defaults:
  - override /dataset: mmimdb
  - override /model: clip_mora

EXP_NOTE: ClipMoRA_MMIMDB

model:
  lora:
    START_LAYER_INDEX: 10
    SHARED_RANK: 4
    VISION_RANK: 4
    TEXT_RANK: 4
    ALPHA: 0.001
    TARGET_MODULES: [
      "clip.vision_model.encoder.layers.{i}.self_attn.q_proj",
      "clip.vision_model.encoder.layers.{i}.self_attn.k_proj",
      "clip.text_model.encoder.layers.{i}.self_attn.q_proj",
      "clip.text_model.encoder.layers.{i}.self_attn.k_proj",
    ]

train:
  EPOCHS: 20
  WEIGHT_DECAY: 2e-2
  BATCH_SIZE: 256